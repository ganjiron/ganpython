{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQLv/d2ZWnfQKKvK4uxyYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganjiron/ganpython/blob/master/CNNwithBayesianOptimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
        "x_train, x_val, x_test = x_train / 255.0, x_val / 255.0, x_test / 255.0\n",
        "y_train, y_val, y_test = to_categorical(y_train), to_categorical(y_val), to_categorical(y_test)"
      ],
      "metadata": {
        "id": "TjmnY6ScsSFe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDEoGzuXtAdq",
        "outputId": "9750196c-09a1-4f0c-9812-091d013da0bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(dropout_rate, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "zFMCU_4ZsSoY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_with(dropout_rate, learning_rate):\n",
        "    model = build_model(dropout_rate, learning_rate)\n",
        "    model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=0)\n",
        "    _, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "h9IaxafhsYig"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 베이지안 최적화 객체 생성\n",
        "optimizer = BayesianOptimization(\n",
        "    f=fit_with,\n",
        "    pbounds={\n",
        "        'dropout_rate': (0.1, 0.5),  # 드롭아웃 비율 범위\n",
        "        'learning_rate': (1e-4, 1e-2)  # 학습률 범위\n",
        "    },\n",
        "    random_state=1234\n",
        ")\n",
        "\n",
        "# 최적화 실행\n",
        "optimizer.maximize(\n",
        "    init_points=2,  # 초기 랜덤 포인트\n",
        "    n_iter=5,       # 최적화 단계 수\n",
        ")\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(optimizer.max)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-gP3RAqsbNh",
        "outputId": "cd2359c4-532f-4ed1-8048-5a5738f1a705"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | dropou... | learni... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.6732   \u001b[0m | \u001b[0m0.1766   \u001b[0m | \u001b[0m0.006259 \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.6424   \u001b[0m | \u001b[0m0.2751   \u001b[0m | \u001b[0m0.007875 \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.6635   \u001b[0m | \u001b[0m0.285    \u001b[0m | \u001b[0m0.003652 \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.6428   \u001b[0m | \u001b[0m0.254    \u001b[0m | \u001b[0m0.005433 \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.6542   \u001b[0m | \u001b[0m0.3978   \u001b[0m | \u001b[0m0.003587 \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m0.6747   \u001b[0m | \u001b[95m0.174    \u001b[0m | \u001b[95m0.006417 \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.6652   \u001b[0m | \u001b[0m0.1581   \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
            "=================================================\n",
            "{'target': 0.6747000217437744, 'params': {'dropout_rate': 0.17400739760630013, 'learning_rate': 0.00641677200574913}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC8SFO61sdww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network (CNN)에서 Convolution Layer(컨볼루션 레이어)를 쌓을 때, 주요 구성 요소와 그들의 의미 및 변화 방법을 아래와 같이 설명할 수 있습니다.\n",
        "\n",
        "1. Filters (필터)\n",
        "\n",
        "\t•\t의미: 필터는 입력 데이터에서 특징을 추출하는 핵심 요소입니다. 각 필터는 입력 데이터의 다른 특징을 학습합니다.\n",
        "\t•\t변화 방법: 초기 레이어에는 적은 수의 필터를 사용하고, 네트워크가 깊어질수록 필터의 수를 늘려가는 것이 일반적입니다. 예를 들어, 첫 번째 Conv 레이어에는 32개의 필터를 사용하고, 다음 레이어에는 64개, 그 다음에는 128개 등으로 늘립니다.\n",
        "\n",
        "2. Kernel Size (커널 크기)\n",
        "\n",
        "\t•\t의미: 커널 크기는 필터의 차원을 결정합니다. 일반적으로 3x3 또는 5x5 크기가 자주 사용됩니다.\n",
        "\t•\t변화 방법: 작은 커널 크기는 더 세밀한 특징을, 큰 커널 크기는 넓은 영역의 특징을 추출합니다. 대부분의 경우 3x3 또는 5x5 크기를 사용합니다.\n",
        "\n",
        "3. Kernel Regularizer (커널 정규화)\n",
        "\n",
        "\t•\t의미: 정규화는 과적합을 방지하는 데 도움이 됩니다. L1, L2 정규화 등이 일반적입니다.\n",
        "\t•\t변화 방법: 정규화를 추가하여 모델이 복잡해지는 것을 방지할 수 있습니다. 예를 들어, kernel_regularizer=tf.keras.regularizers.l2(0.01)과 같이 사용합니다.\n",
        "\n",
        "4. Pooling (풀링)\n",
        "\n",
        "\t•\t의미: 풀링은 특징 맵의 크기를 줄이면서 중요한 정보를 유지합니다. Max Pooling과 Average Pooling이 일반적입니다.\n",
        "\t•\t변화 방법: 일반적으로 컨볼루션 레이어 다음에 풀링 레이어를 추가합니다. 예를들어, MaxPooling2D(pool_size=(2, 2))는 2x2 크기의 윈도우로 최대값을 추출하는 맥스 풀링을 적용합니다.\n",
        "\n",
        "5. Activation (활성화 함수)\n",
        "\n",
        "\t•\t의미: 활성화 함수는 신경망에 비선형성을 도입합니다. 이는 모델이 복잡한 패턴을 학습할 수 있게 해줍니다.\n",
        "\t•\t변화 방법: ReLU는 가장 일반적으로 사용되는 활성화 함수입니다. 깊은 레이어에서는 LeakyReLU나 ELU 같은 변형된 형태를 사용하기도 합니다.\n",
        "\n",
        "6. Dropout Rate (드롭아웃 비율)\n",
        "\n",
        "\t•\t의미: 드롭아웃은 훈련 과정에서 무작위로 뉴런의 일부를 비활성화하여 과적합을 방지합니다.\n",
        "\t•\t변화 방법: 드롭아웃 비율은 일반적으로 0.2에서 0.5 사이입니다. 과적합이 관찰되면 드롭아웃 비율을 증가시킬 수 있습니다.\n",
        "\n",
        "일반적인 CNN 구조에서 이러한 요소들의 변화\n",
        "\n",
        "\t•\t초기 레이어: 적은 수의 필터와 작은 커널 크기로 시작하여 세밀한 특징을 추출합니다. 드롭아웃은 일반적으로 사용하지 않거나 낮은 비율을 사용합니다.\n",
        "\t•\t중간 레이어: 필터의 수를 증가시키고, 필요에 따라 커널 크기를 조정합니다. 정규화 또는 드롭아웃을 추가하여 과적합을 방지합니다.\n",
        "\t•\t마지막 레이어: 클래스 분류를 위해 Flatten 후 Dense 레이어를 추가합니다. 이 때 Softmax 활성화 함수(다중 분류) 또는 Sigmoid 활성화 함수(이진 분류)를 사용합니다.\n",
        "\n",
        "이러한 요소들을 적절히 조정하면 다양한 데이터와 문제에 맞는 효과적인 CNN 모델을 구축할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "iI171Te_1n0H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nf0XzMC1-Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1000개의 사진을 이용하여 CNN(Convolutional Neural Network) 모델을 구축할 때 적절한 레이어의 수는 여러 요인에 따라 달라질 수 있습니다. 아래 요인들을 고려하여 결정하는 것이 중요합니다:\n",
        "\n",
        "1. 이미지의 복잡성\n",
        "\n",
        "\t•\t간단한 이미지: 단순한 이미지(예: 숫자 인식)의 경우, 몇 개의 컨볼루션 레이어만으로도 충분할 수 있습니다.\n",
        "\t•\t복잡한 이미지: 복잡한 이미지(예: 자연 경관)나 다양한 객체를 포함하는 경우, 더 많은 레이어가 필요할 수 있습니다.\n",
        "\n",
        "2. 데이터 양\n",
        "\n",
        "\t•\t1000개의 이미지는 상대적으로 적은 양입니다. 너무 깊은 네트워크는 과적합(overfitting)을 일으킬 수 있습니다. 따라서 단순한 구조를 시작점으로 삼고 필요에 따라 점차 복잡성을 늘리는 것이 좋습니다.\n",
        "\n",
        "3. 과적합 방지 전략\n",
        "\n",
        "\t•\t데이터 증강(augmentation), 드롭아웃(dropout), 정규화(regularization)와 같은 과적합 방지 기법을 적용하여, 더 깊은 모델을 사용할 수 있습니다.\n",
        "\n",
        "4. 실험과 조정\n",
        "\n",
        "\t•\t레이어의 수를 조정하며 실험을 여러 번 해보고, 검증 데이터셋에서의 성능을 확인합니다. 처음에는 간단한 모델로 시작하여 점차 복잡도를 늘려가며 최적의 구조를 찾습니다.\n",
        "\n",
        "일반적인 권장 사항\n",
        "\n",
        "\t•\t초기 모델: 2-3개의 컨볼루션 레이어를 포함하는 간단한 모델로 시작합니다. 각 컨볼루션 레이어 뒤에는 활성화 함수(예: ReLU)와 풀링 레이어(MaxPooling)를 추가할 수 있습니다.\n",
        "\n",
        "\t•\t중간 레이어: 필요에 따라 추가 컨볼루션 레이어를 추가하며, 각 레이어의 필터 수를 점차 늘려갑니다. 예를 들어, 첫 번째 레이어에서 32개의 필터를 사용한 후, 다음 레이어에서는 64개, 그 다음 레이어에서는 128개의 필터를 사용할 수 있습니다.\n",
        "\t•\t드롭아웃과 정규화: 과적합을 방지하기 위해 드롭아웃 레이어를 추가하거나, 컨볼루션 레이어에 L1 또는 L2 정규화를 적용할 수 있습니다.\n",
        "\t•\t완전 연결 레이어(Fully Connected Layer): 마지막에 하나 또는 두 개의 완전 연결 레이어(Dense Layer)를 추가하여 분류 작업을 수행합니다.\n",
        "\n",
        "예시\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "이 예시에서는 두 개의 컨볼루션 레이어를 사용하고 있으며, 각 레이어 뒤에 맥스 풀링 레이어가 있습니다. 데이터 양이 비교적 적기 때문에, 모델이 너무 복잡하지 않도록 유의하는 것이 중요합니다."
      ],
      "metadata": {
        "id": "6o7m6kWq3PJx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKzXEqxw32JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미 훈련된 모델에 새로운 데이터를 지속적으로 추가하면서 학습을 계속하는 방법은 “점진적 학습” 또는 “지속적 학습(continuous learning)“이라고 불리며, 특정 상황에서 유용할 수 있습니다. 이 방식은 특히 데이터가 시간에 따라 점진적으로 수집되는 경우나, 새로운 종류의 데이터를 모델에 통합해야 할 때 적합합니다.\n",
        "\n",
        "장점\n",
        "\n",
        "\t1.\t데이터 활용 최대화: 새로운 데이터가 수집될 때마다 이를 학습에 활용하여 모델의 성능을 개선할 수 있습니다.\n",
        "\t2.\t시간과 자원 절약: 처음부터 모델을 다시 훈련시키지 않고, 기존에 학습된 가중치를 기반으로 추가 학습을 진행할 수 있어 시간과 자원을 절약할 수 있습니다.\n",
        "\t3.\t새로운 특징 학습: 새로운 데이터에 포함된 새로운 특징이나 패턴을 모델이 학습할 수 있습니다.\n",
        "\n",
        "단점 및 고려사항\n",
        "\n",
        "\t1.\t과적합의 위험: 새로운 데이터에만 지나치게 특화되어 기존에 학습한 데이터에 대한 성능이 저하될 수 있습니다. 이를 “카타스트로픽 포기팅(catastrophic forgetting)“이라고 합니다.\n",
        "\t2.\t데이터 분포의 변화: 새로운 데이터가 기존 데이터와 분포가 다를 경우, 모델이 이전 데이터에서 학습한 정보를 “잊어버릴” 수 있습니다.\n",
        "\t3.\t지속적인 모니터링 필요: 모델이 계속해서 좋은 성능을 유지하고 있는지 정기적으로 확인하고, 필요에 따라 하이퍼파라미터를 조정해야 합니다.\n",
        "\n",
        "접근 방법\n",
        "\n",
        "\t•\t데이터 증강과 병합: 새로운 데이터와 기존 데이터를 적절히 혼합하여 사용하면, 과적합과 카타스트로픽 포기팅을 어느 정도 방지할 수 있습니다.\n",
        "\t•\t학습률 조정: 추가 학습 시학습률을 낮추어 기존에 학습한 가중치를 과도하게 변경하지 않도록 조심스럽게 접근하는 것이 좋습니다. 너무 높은 학습률은 기존에 학습한 특징을 빠르게 잊게 만들 수 있습니다.\n",
        "\n",
        "\t•\t적절한 평가: 지속적인 학습 과정에서는 새로운 데이터 뿐만 아니라 기존 데이터에 대해서도 모델의 성능을 주기적으로 평가해야 합니다. 이를 통해 모델이 전체 데이터셋에 대해 균형 잡힌 성능을 유지하고 있는지 확인할 수 있습니다.\n",
        "\t•\t분할 학습: 새로운 데이터가 매우 다른 특징을 가지고 있다면, 별도의 모델을 훈련시키고 이를 기존 모델과 앙상블하는 방법도 고려할 수 있습니다.\n",
        "\n",
        "결론\n",
        "\n",
        "지속적으로 데이터가 추가될 때, 기존 모델에 새로운 데이터를 반복적으로 학습시키는 것은 유용할 수 있지만, 과적합, 카타스트로픽 포기팅, 데이터 분포의 변화 등에 주의해야 합니다. 적절한 학습률 설정, 데이터 증강, 주기적인 평가를 통해 이러한 문제를 최소화할 수 있습니다."
      ],
      "metadata": {
        "id": "phCnp2bt7qxh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ei2xV9f7uVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}