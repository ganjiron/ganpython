{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-2_Spark_RDD_and_SQL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "K9JzWDhUXkh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Software Platform Lab, Seoul National University\n",
        "\n",
        "## Colab 101\n",
        "\n",
        "Colab is a free Jupyter notebook environment by Google Research. Unlike AWS cluster (which is charged every hour it is up and running), you can run experiments on your own environment.\n",
        "\n",
        "## Colab Spark Setup"
      ]
    },
    {
      "metadata": {
        "id": "cGKLcJu7HoB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl http://mirror.cogentco.com/pub/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz --output spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSL4Z8LJHp4F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-Pvn8rkXuHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wikipedia dataset sample\n",
        "\n",
        "This time we're not using HDFS to load the data. Sample data are loaded by Python code directly.\n",
        "\n",
        "The data has four fields: project, title, pageview count and size."
      ]
    },
    {
      "metadata": {
        "id": "HTYHQ4OCU4LD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikipedia_data_sample = [\"commons.m File:Gemblong.JPG 1 9717\"\n",
        ",\"pl Beata_Tyszkiewicz 10 207378\"\n",
        ",\"en Special:RecentChangesLinked/Roswell_(TV_series) 1 14617\"\n",
        ",\"de Grafische_Benutzeroberfl%C3%A4che 1 22549\"\n",
        ",\"en Simeon_I_of_Bulgaria 5 385793\"\n",
        ",\"en Rainbow_Six_(novel) 8 122792\"\n",
        ",\"es Pediatr%C3%ADa 5 73598\"\n",
        ",\"sv Ett_uts%C3%B6kt_universum 1 9499\"\n",
        ",\"en Video_game_content_rating_system 4 112324\"\n",
        ",\"es Yuno_Gasai 2 55260\"\n",
        ",\"en File:Georg_Wilhelm_Friedrich_Hegel00.jpg 1 43395\"\n",
        ",\"en Anestia_ombrophanes 1 8881\"\n",
        ",\"et Seitse 2 84874\"\n",
        ",\"en And_I_Am_Telling_You_I%27m_Not_Going 4 85690\"\n",
        ",\"he %D7%A4%D7%A8%D7%93%D7%99%D7%92%D7%9E%D7%94 1 13887\"\n",
        ",\"zh File:Pictogram_voting_keep-green.svg 1 15106\"\n",
        ",\"sv Special:Senaste_relaterade_%C3%A4ndringar/Homestead,_Florida 1 7677\"\n",
        ",\"pt Categoria:Ambientes_de_desenvolvimento_integrado_livres 1 8151\"\n",
        ",\"de.voy Plattensee 1 43748\"\n",
        ",\"en Independent_Chip_Model 1 8938\"\n",
        ",\"en Category:Toronto_Toros_players 2 0\"\n",
        ",\"en Special:Export/Helsinki_Accords 1 19899\"\n",
        ",\"xh Special:Contributions/Kpeterzell 1 5883\"\n",
        ",\"nl 4_mei 1 0\"\n",
        ",\"no Carlos_Keller_Rueff 5 87075\"\n",
        ",\"en Special:Contributions/2.31.218.202 1 7402\"\n",
        ",\"es Placa_Yangtze 1 10329\"\n",
        ",\"de Datei:BSicon_uhKBHFe.svg 1 9786\"\n",
        ",\"en Randolph_County,_Alabama 1 21431\"\n",
        ",\"es S%C3%A9neca 3 70494\"\n",
        ",\"en Tu_Bishvat 3 56438\"\n",
        ",\"cs Radiohead 1 14325\"\n",
        ",\"es Naturaleza_sangre 1 9286\"\n",
        ",\"en Anatolia_(disambiguation) 1 7980\"\n",
        ",\"pt Queima_de_suti%C3%A3s 1 8982\"\n",
        ",\"pt Titanoboa_cerrejonensis 5 64540\"\n",
        ",\"commons.m Category:People_of_Ireland 1 19278\"\n",
        ",\"fi Matti_Inkinen 1 10138\"\n",
        ",\"ja %E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Esfahan_(Iran)_Emam_Mosque.JPG 1 33168\"\n",
        ",\"en Psicobloc 1 12739\"\n",
        ",\"en Macael,_Spain 1 12658\"\n",
        ",\"fa %DA%A9%D9%87%D8%AA%D9%88%DB%8C%D9%87 1 22855\"\n",
        ",\"fr Sp%C3%A9cial:Pages_li%C3%A9es/Fichier:Wiki-ezokuroten5.jpg 1 21955\"\n",
        ",\"nl Overleg_gebruiker:82.171.157.232 1 0\"\n",
        ",\"en Thomas_%26_Mack_Center 2 41010\"\n",
        ",\"en Warren_Beatty 49 2631986\"\n",
        ",\"uz Auberville 1 11401\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TG9kePIjX6A5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Spark RDD Transforms\n",
        "\n",
        "Now we'll try several Spark RDD transforms using the sample wikipedia dataset."
      ]
    },
    {
      "metadata": {
        "id": "6n8go8ojHp_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = ss.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZx1MpvJcdI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parallelize the data and split into columns\n",
        "lines = sc.parallelize(wikipedia_data_sample)\n",
        "columns = lines.map(lambda line: tuple(line.split(\" \")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPcVG0I33j4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Element-Wise Transformation: Map Transform\n",
        "\n",
        "# Create (project, count) tuples(Be mindful of 'long()'!)\n",
        "project_count_tuples = columns.map(lambda column: (column[0], long(column[2])))\n",
        "project_count_tuples.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SADcMkDj3OVc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Element-Wise Transformation: Filter Transform\n",
        "\n",
        "# Filter project containing name 'de'\n",
        "project_de_filtered = project_count_tuples.filter(lambda t: 'de' in t[0])\n",
        "project_de_filtered.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlUGGF-hbzeu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 1\n",
        "Sample wikipedia data에서 project 의 count column 값이 5 이상인 경우를 filter 하시오.\n",
        "- 결과값: project, count 로 구성된 tuple"
      ]
    },
    {
      "metadata": {
        "id": "hYlIksygDDjD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Element-Wise Transformation: Filter Transform\n",
        "\n",
        "# Code for the quiz 1 here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhZ3zYpqWiy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Transformations on one Pair RDD: ReduceByKey Transform\n",
        "\n",
        "# Compute the sum of pageview counts per project\n",
        "project_sum_tuples = project_count_tuples.reduceByKey(lambda left, right: left + right) \n",
        "project_sum_tuples.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IVcZtTxoJvGm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Transformations on two Pair RDDs: Join Transform\n",
        "\n",
        "# Declare another two sample data\n",
        "wikipedia_sample_singer = [\"en Steve_Jobs 49 2631986\"\n",
        ",\"en WoodKid 1 12739\"\n",
        ",\"en Honne 100 12658\"\n",
        ",\"fa Eminem 1 22855\"\n",
        ",\"en Sia 49 2631986\"]\n",
        "\n",
        "singer_to_ranking = [\"WoodKid 1\"\n",
        ",\"Honne 2\"\n",
        ",\"Eminem 3\"\n",
        ",\"Sia 4\"]\n",
        "\n",
        "# Parallelize the data and split into columns\n",
        "lines2 = sc.parallelize(wikipedia_sample_singer)\n",
        "lines3 = sc.parallelize(singer_to_ranking)\n",
        "\n",
        "wikipedia_sample_singer_tuples = lines2.map(lambda line: tuple(line.split(\" \")))\n",
        "singer_to_ranking_tuples = lines3.map(lambda line: tuple(line.split(\" \")))\n",
        "\n",
        "# Create (title, count) tuples and join via title name.\n",
        "title_count_tuples = wikipedia_sample_singer_tuples.map(lambda column: (column[1], long(column[2])))\n",
        "title_count_tuples.join(singer_to_ranking_tuples).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgEM6fA2YaPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SparkSQL"
      ]
    },
    {
      "metadata": {
        "id": "Z650qgjHTFLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame from wikipedia_data_sample (equivalent of a 'SQL table' in Spark)\n",
        "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view called \"WikipediaTable\"\n",
        "df.createOrReplaceTempView(\"WikipediaTable\")\n",
        "\n",
        "# Run an SQL query that selects project equals to 'en' with count greater than 5\n",
        "selected = ss.sql(\"SELECT project, count FROM WikipediaTable \\\n",
        "                   WHERE project='en' AND count >= 5\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aw3J1GToUxl7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run an SQL query that orders projects by the number of titles each project has\n",
        "selected = ss.sql(\"SELECT project, COUNT(title) as num_of_title FROM WikipediaTable \\\n",
        "                  GROUP BY project \\\n",
        "                  ORDER BY num_of_title DESC\")\n",
        "\n",
        "# Print the results in this console (top 20 results will be shown)\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iN-tZTf_RasG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame from singer_to_ranking and wikipedia_sample_singer\n",
        "df = ss.createDataFrame(singer_to_ranking_tuples, ['title', 'ranking'])\n",
        "df1 = ss.createDataFrame(wikipedia_sample_singer_tuples, ['project', 'title', 'count', 'size'])\n",
        "\n",
        "# Create a table view of them, called \"RankingTable\" and \"SingerTable\"\n",
        "df.createOrReplaceTempView(\"RankingTable\")\n",
        "df1.createOrReplaceTempView(\"SingerTable\")\n",
        "\n",
        "# Run an SQL query that joins the two tables.\n",
        "# The result will show 'ranking' of RankingTable and 'title', 'count' of SingerTable.\n",
        "# Join will be performed on rows with common 'title' in both tables.\n",
        "selected = ss.sql(\"SELECT RankingTable.ranking, SingerTable.title, SingerTable.count FROM SingerTable \\\n",
        "                   INNER JOIN RankingTable ON RankingTable.title=SingerTable.title \\\n",
        "                   ORDER BY RankingTable.ranking\")\n",
        "\n",
        "selected.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2vgvCH5STk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FzKCclPWYmG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 2. \n",
        "'WikipediaTable'에서, 각 project 당 count column 값의 총합이 20 이상인 (project, sum_of_count)를 구하시오\n",
        "- 결과값: project, sum_of_count 2개의 column 을 갖는 테이블"
      ]
    },
    {
      "metadata": {
        "id": "sp9V8qXNY5CS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Code for the quiz 1 here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OLQZBBAiY_UN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz 3.\n",
        "다음의 table을 'WikipediaTable'과 Join하여, grade가 'C'에 해당하는 project에 속하는 title들을 구하시오\n",
        "- 결과값: title 1개의 column 을 갖는 테이블"
      ]
    },
    {
      "metadata": {
        "id": "Zm7cN8MuZNPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cols = ['project', 'grade']\n",
        "vals = [\n",
        "     ('en', 'C'),\n",
        "     ('he', 'A'),\n",
        "     ('zh', 'B'),    \n",
        "     ('no', 'A')\n",
        "]\n",
        "\n",
        "title_grade = ss.createDataFrame(vals, cols)\n",
        "title_grade.show()\n",
        "title_grade.createOrReplaceTempView(\"TitleGradeTable\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EnpkIUQbTKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Code for the quiz 2 here!\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IfTZ9a3pbYOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}