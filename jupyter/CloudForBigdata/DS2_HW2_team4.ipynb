{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS2_HW2_team4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vzg7GMj_Ikb2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright (C) 2018 Software Platform Lab, Seoul National University\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "you may not use this file except in compliance with the License.\n",
        "\n",
        "\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "    \n",
        "    \n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "\n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "Tv1ZcltrbkFI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DS2_HW2: Training CIFAR-10 with Resnet50 model\n",
        "Train CIFAR-10 dataset with Resnet50 model, save checkpoint and evaluate the model.\n",
        "- Create input pipeline using Tensorflow Dataset API\n",
        "- Define optimizer for train\n",
        "- Save checkpoints while training, and use those checkpoints for evaluation.\n",
        "- Add summaries on Tensorboard."
      ]
    },
    {
      "metadata": {
        "id": "gu9cVjyfbkz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "cellView": "form",
        "outputId": "8d3b644e-ba4a-4253-b91f-9d0e14b3c7cc"
      },
      "cell_type": "code",
      "source": [
        "#@title Run me to download the CIFAR-10 dataset!\n",
        "# https://blog.shichao.io/2012/10/04/progress_speed_indicator_for_urlretrieve_in_python.html\n",
        "\n",
        "import os, sys, time\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "  global start_time\n",
        "  if count == 0:\n",
        "    start_time = time.time()\n",
        "    return\n",
        "  duration = time.time() - start_time\n",
        "  progress_size = int(count * block_size)\n",
        "  percent = int(count * block_size * 100 / total_size)\n",
        "  sys.stdout.write('\\r...%d%%, %d MB, %d seconds passed' %\n",
        "                   (percent, progress_size / (1024 * 1024), duration))\n",
        "  sys.stdout.flush()\n",
        "\n",
        "cifar10url = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "cifar10 = cifar10url.split('/')[-1]\n",
        "\n",
        "if not os.path.isfile(cifar10):\n",
        "  urllib.urlretrieve(cifar10url, cifar10, reporthook)\n",
        "print()\n",
        "print('Download finished!')\n",
        "\n",
        "cifar10_extracted = 'cifar-10-batches-bin'\n",
        "\n",
        "if not os.path.isdir(cifar10_extracted):\n",
        "  tarfile.open(cifar10, 'r:gz').extractall()\n",
        "print('Uncompression finished!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "Download finished!\n",
            "Uncompression finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2o4xeczHddip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b544adb-d836-4a79-f773-e78bd3e3ea62"
      },
      "cell_type": "code",
      "source": [
        "!mkdir train_ckpt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train_ckpt’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JBGRBH9fg742",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc226646-e3de-44c1-9e89-e5b9b1a54aef"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-bin  cifar-10-binary.tar.gz  sample_data  train_ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "47vs_zxwbzgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Resnet50 Model"
      ]
    },
    {
      "metadata": {
        "id": "cL5o4WI7b2-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"ResNet model.\n",
        "\n",
        "Related papers:\n",
        "https://arxiv.org/pdf/1603.05027v2.pdf\n",
        "https://arxiv.org/pdf/1512.03385v1.pdf\n",
        "https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import six\n",
        "\n",
        "from tensorflow.python.training import moving_averages\n",
        "\n",
        "\n",
        "HParams = namedtuple('HParams',\n",
        "                     'batch_size, num_classes, min_lrn_rate, lrn_rate, '\n",
        "                     'num_residual_units, use_bottleneck, weight_decay_rate, '\n",
        "                     'relu_leakiness')\n",
        "\n",
        "\n",
        "class ResNet(object):\n",
        "  \"\"\"ResNet model.\"\"\"\n",
        "\n",
        "  def __init__(self, hps, images, labels, mode):\n",
        "    \"\"\"ResNet constructor.\n",
        "\n",
        "    Args:\n",
        "      hps: Hyperparameters.\n",
        "      images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "      labels: Batches of labels. [batch_size, num_classes]\n",
        "      mode: One of 'train' and 'eval'.\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "    self._images = images\n",
        "    self.labels = labels\n",
        "    self.mode = mode\n",
        "\n",
        "    self._extra_train_ops = []\n",
        "\n",
        "  def build_graph(self):\n",
        "    \"\"\"Build a whole graph for the model.\"\"\"\n",
        "    self.global_step = tf.train.get_or_create_global_step()\n",
        "    self._build_model()\n",
        "    if self.mode == 'train':\n",
        "      self._build_train_op()\n",
        "    self.summaries = tf.summary.merge_all()\n",
        "\n",
        "  def _stride_arr(self, stride):\n",
        "    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n",
        "    return [1, stride, stride, 1]\n",
        "\n",
        "  def _build_model(self):\n",
        "    \"\"\"Build the core model within the graph.\"\"\"\n",
        "    with tf.variable_scope('init'):\n",
        "      x = self._images\n",
        "      x = self._conv('init_conv', x, 3, 3, 16, self._stride_arr(1))\n",
        "\n",
        "    strides = [1, 2, 2]\n",
        "    activate_before_residual = [True, False, False]\n",
        "    if self.hps.use_bottleneck:\n",
        "      res_func = self._bottleneck_residual\n",
        "      filters = [16, 64, 128, 256]\n",
        "    else:\n",
        "      res_func = self._residual\n",
        "      filters = [16, 16, 32, 64]\n",
        "      # Uncomment the following codes to use w28-10 wide residual network.\n",
        "      # It is more memory efficient than very deep residual network and has\n",
        "      # comparably good performance.\n",
        "      # https://arxiv.org/pdf/1605.07146v1.pdf\n",
        "      # filters = [16, 160, 320, 640]\n",
        "      # Update hps.num_residual_units to 4\n",
        "\n",
        "    with tf.variable_scope('unit_1_0'):\n",
        "      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n",
        "                   activate_before_residual[0])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_1_%d' % i):\n",
        "        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_2_0'):\n",
        "      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n",
        "                   activate_before_residual[1])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_2_%d' % i):\n",
        "        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_3_0'):\n",
        "      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
        "                   activate_before_residual[2])\n",
        "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
        "      with tf.variable_scope('unit_3_%d' % i):\n",
        "        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
        "\n",
        "    with tf.variable_scope('unit_last'):\n",
        "      x = self._batch_norm('final_bn', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._global_avg_pool(x)\n",
        "\n",
        "    with tf.variable_scope('logit'):\n",
        "      logits = self._fully_connected(x, self.hps.num_classes)\n",
        "      self.predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    with tf.variable_scope('costs'):\n",
        "      xent = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=self.labels)\n",
        "      self.cost = tf.reduce_mean(xent, name='xent')\n",
        "      self.cost += self._decay()\n",
        "\n",
        "      tf.summary.scalar('cost', self.cost)\n",
        "\n",
        "  def _build_train_op(self):\n",
        "    \"\"\"Build training specific ops for the graph.\"\"\"\n",
        "    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\n",
        "    tf.summary.scalar('learning_rate', self.lrn_rate)\n",
        "\n",
        "    trainable_variables = tf.trainable_variables()\n",
        "    grads = tf.gradients(self.cost, trainable_variables)\n",
        "\n",
        "    \n",
        "    #########################################################################\n",
        "    #### FIXME: Create an optimizer using self.lrn_rate as learning rate ####\n",
        "    #########################################################################\n",
        "#     optimizer = tf.train.AdamOptimizer(self.lrn_rate)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n",
        "\n",
        "    apply_op = optimizer.apply_gradients(\n",
        "        zip(grads, trainable_variables),\n",
        "        name='train_step')\n",
        "    with tf.control_dependencies([apply_op]):\n",
        "      apply_op = tf.assign_add(self.global_step, 1)\n",
        "   \n",
        "    train_ops = [apply_op] + self._extra_train_ops\n",
        "    self.train_op = tf.group(*train_ops)\n",
        "\n",
        "  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n",
        "  def _batch_norm(self, name, x):\n",
        "    \"\"\"Batch normalization.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      params_shape = [x.get_shape()[-1]]\n",
        "\n",
        "      beta = tf.get_variable(\n",
        "          'beta', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "      gamma = tf.get_variable(\n",
        "          'gamma', params_shape, tf.float32,\n",
        "          initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "      if self.mode == 'train':\n",
        "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
        "\n",
        "        moving_mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        moving_variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_mean, mean, 0.9))\n",
        "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
        "            moving_variance, variance, 0.9))\n",
        "      else:\n",
        "        mean = tf.get_variable(\n",
        "            'moving_mean', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(0.0, tf.float32),\n",
        "            trainable=False)\n",
        "        variance = tf.get_variable(\n",
        "            'moving_variance', params_shape, tf.float32,\n",
        "            initializer=tf.constant_initializer(1.0, tf.float32),\n",
        "            trainable=False)\n",
        "        tf.summary.histogram(mean.op.name, mean)\n",
        "        tf.summary.histogram(variance.op.name, variance)\n",
        "      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n",
        "      y = tf.nn.batch_normalization(\n",
        "          x, mean, variance, beta, gamma, 0.001)\n",
        "      y.set_shape(x.get_shape())\n",
        "      return y\n",
        "\n",
        "  def _residual(self, x, in_filter, out_filter, stride,\n",
        "                activate_before_residual=False):\n",
        "    \"\"\"Residual unit with 2 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('shared_activation'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_only_activation'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
        "        orig_x = tf.pad(\n",
        "            orig_x, [[0, 0], [0, 0], [0, 0],\n",
        "                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.debug('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n",
        "                           activate_before_residual=False):\n",
        "    \"\"\"Bottleneck residual unit with 3 sub layers.\"\"\"\n",
        "    if activate_before_residual:\n",
        "      with tf.variable_scope('common_bn_relu'):\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "        orig_x = x\n",
        "    else:\n",
        "      with tf.variable_scope('residual_bn_relu'):\n",
        "        orig_x = x\n",
        "        x = self._batch_norm('init_bn', x)\n",
        "        x = self._relu(x, self.hps.relu_leakiness)\n",
        "\n",
        "    with tf.variable_scope('sub1'):\n",
        "      x = self._conv('conv1', x, 1, in_filter, out_filter/4, stride)\n",
        "\n",
        "    with tf.variable_scope('sub2'):\n",
        "      x = self._batch_norm('bn2', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv2', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub3'):\n",
        "      x = self._batch_norm('bn3', x)\n",
        "      x = self._relu(x, self.hps.relu_leakiness)\n",
        "      x = self._conv('conv3', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n",
        "\n",
        "    with tf.variable_scope('sub_add'):\n",
        "      if in_filter != out_filter:\n",
        "        orig_x = self._conv('project', orig_x, 1, in_filter, out_filter, stride)\n",
        "      x += orig_x\n",
        "\n",
        "    tf.logging.info('image after unit %s', x.get_shape())\n",
        "    return x\n",
        "\n",
        "  def _decay(self):\n",
        "    \"\"\"L2 weight decay loss.\"\"\"\n",
        "    costs = []\n",
        "    for var in tf.trainable_variables():\n",
        "      if var.op.name.find(r'DW') > 0:\n",
        "        costs.append(tf.nn.l2_loss(var))\n",
        "        # tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
        "\n",
        "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
        "    \"\"\"Convolution.\"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "      n = filter_size * filter_size * out_filters\n",
        "      kernel = tf.get_variable(\n",
        "          'DW', [filter_size, filter_size, in_filters, out_filters],\n",
        "          tf.float32, initializer=tf.random_normal_initializer(\n",
        "              stddev=np.sqrt(2.0/n)))\n",
        "      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
        "\n",
        "  def _relu(self, x, leakiness=0.0):\n",
        "    \"\"\"Relu, with optional leaky support.\"\"\"\n",
        "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
        "\n",
        "  def _fully_connected(self, x, out_dim):\n",
        "    \"\"\"FullyConnected layer for final output.\"\"\"\n",
        "    x = tf.reshape(x, [self.hps.batch_size, -1])\n",
        "    w = tf.get_variable(\n",
        "        'DW', [x.get_shape()[1], out_dim],\n",
        "        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    b = tf.get_variable('biases', [out_dim],\n",
        "                        initializer=tf.constant_initializer())\n",
        "    return tf.nn.xw_plus_b(x, w, b)\n",
        "\n",
        "  def _global_avg_pool(self, x):\n",
        "    assert x.get_shape().ndims == 4\n",
        "    return tf.reduce_mean(x, [1, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cc0UJQMReBnw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CIFAR10 input"
      ]
    },
    {
      "metadata": {
        "id": "mv5rNbsKeBc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"CIFAR dataset input module.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_input(dataset, data_path, batch_size, mode):\n",
        "  \"\"\"Build CIFAR image and labels.\n",
        "\n",
        "  Args:\n",
        "    dataset: Either 'cifar10' or 'cifar100'.\n",
        "    data_path: Filename for data.\n",
        "    batch_size: Input batch size.\n",
        "    mode: Either 'train' or 'eval'.\n",
        "  Returns:\n",
        "    images: Batches of images. [batch_size, image_size, image_size, 3]\n",
        "    labels: Batches of labels. [batch_size, num_classes]\n",
        "  Raises:\n",
        "    ValueError: when the specified dataset is not supported.\n",
        "  \"\"\"\n",
        "  image_size = 32\n",
        "  if dataset == 'cifar10':\n",
        "    label_bytes = 1\n",
        "    label_offset = 0\n",
        "    num_classes = 10\n",
        "  elif dataset == 'cifar100':\n",
        "    label_bytes = 1\n",
        "    label_offset = 1\n",
        "    num_classes = 100\n",
        "  else:\n",
        "    raise ValueError('Not supported dataset %s', dataset)\n",
        "\n",
        "  depth = 3\n",
        "  image_bytes = image_size * image_size * depth\n",
        "  record_bytes = label_bytes + label_offset + image_bytes\n",
        "\n",
        "  def parse_data(value): \n",
        "    # Convert these examples to dense labels and processed images.\n",
        "    record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n",
        "    label = tf.cast(tf.slice(record, [label_offset], [label_bytes]), tf.int32)\n",
        "\n",
        "    # Convert from string to [depth * height * width] to [depth, height, width].\n",
        "    depth_major = tf.reshape(tf.slice(record, [label_offset + label_bytes], [image_bytes]),\n",
        "                           [depth, image_size, image_size])\n",
        "    # Convert from [depth, height, width] to [height, width, depth].\n",
        "    image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
        "\n",
        "    if mode == 'train':\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size+4, image_size+4)\n",
        "      image = tf.random_crop(image, [image_size, image_size, 3])\n",
        "      image = tf.image.random_flip_left_right(image)\n",
        "      # Brightness/saturation/constrast provides small gains .2%~.5% on cifar.\n",
        "      # image = tf.image.random_brightness(image, max_delta=63. / 255.)\n",
        "      # image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "      # image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    else:\n",
        "      image = tf.image.resize_image_with_crop_or_pad(\n",
        "        image, image_size, image_size)\n",
        "      image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "  data_files = tf.gfile.Glob(data_path)\n",
        "  data_files.sort()\n",
        "  ds = tf.data.Dataset.from_tensor_slices(data_files)\n",
        "  #############################################################################\n",
        "  #### FIXME: Create an input pipline using tf.data.Dataset and parse_data ####\n",
        "  #############################################################################\n",
        "#   ??? \n",
        "#   iterator = ???\n",
        "  ds = tf.data.FixedLengthRecordDataset(data_files,record_bytes)\n",
        "  ds = ds.map(parse_data)\n",
        "  # Dataset.shuffle: shuffle data\n",
        "  ds = ds.shuffle(10000)\n",
        "  # Dataset.repeat: repeate the dataset (not only 1 epoch)\n",
        "  ds = ds.repeat()\n",
        "  # Dataset.batch: Combine elements to generate a batch of data\n",
        "  ds = ds.batch(batch_size)\n",
        "  iterator = ds.make_one_shot_iterator()\n",
        "  \n",
        "  images, labels = iterator.get_next()\n",
        "  \n",
        "\n",
        "  images = tf.reshape(images, [batch_size, image_size, image_size, depth])\n",
        "  labels = tf.reshape(labels, [batch_size, 1])\n",
        "  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
        "  labels = tf.sparse_to_dense(\n",
        "      tf.concat(values=[indices, labels], axis=1),\n",
        "      [batch_size, num_classes], 1.0, 0.0) \n",
        "  return images, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYszRyxjb2aZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Resnet Train\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "metadata": {
        "id": "sR7jlpOUdQhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "9bedc36a-81c3-4a4a-f19a-f531a90a9a72"
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global\n",
        "train_data_path = './cifar-10-batches-bin/data_batch*'\n",
        "image_size = 32\n",
        "ckpt_dir = './train_ckpt'\n",
        "ckpt_prefix = ckpt_dir + '/cifar10-train'\n",
        "\n",
        "def train(hps , batch_size): #batch_size 넣음\n",
        "  \"\"\"Training loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "      'cifar10', train_data_path, hps.batch_size, 'train')\n",
        "    model = ResNet(hps, images, labels, 'train')\n",
        "    model.build_graph()\n",
        "\n",
        "    truth = tf.argmax(model.labels, axis=1)\n",
        "    predictions = tf.argmax(model.predictions, axis=1)\n",
        "    precision = tf.reduce_mean(tf.to_float(tf.equal(predictions, truth)))\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    ###########################################\n",
        "    #### FIXME: Create an checkpoint Saver ####\n",
        "    #### WARNING: use max_to_keep arg      ####\n",
        "    ###########################################\n",
        "    saver = tf.train.Saver(max_to_keep=30)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for i in range(3001):\n",
        "        _, global_step, cost, precision_ = \\\n",
        "          sess.run([model.train_op, model.global_step, model.cost, precision])\n",
        "\n",
        "        if global_step % 100 == 0:\n",
        "#           elapsed = time.time() - start # 시간 추가\n",
        "#           print('step: %d, loss: %.3f, precision: %.3f , throughput = %.3f img/sec' % (global_step, cost, precision_ , 100 * batch_size / elapsed))\n",
        "          print('step: %d, loss: %.3f, precision: %.3f ' % (global_step, cost, precision_ ))\n",
        "          #################################################\n",
        "          #### FIXME: Save the model using Saver       ####\n",
        "          #### Use ckpt_prefix as checkpoint save path ####\n",
        "          #################################################\n",
        "          saver.save(sess, ckpt_prefix , global_step=global_step)\n",
        "#           start = time.time()\n",
        "\n",
        "        \n",
        "def main(_):\n",
        "  batch_size = 128\n",
        "\n",
        "  hps = HParams(batch_size=batch_size,\n",
        "                             num_classes=10,\n",
        "                             min_lrn_rate=0.0001,\n",
        "                             lrn_rate=0.1,\n",
        "                             num_residual_units=5,\n",
        "                             use_bottleneck=False,\n",
        "                             weight_decay_rate=0.0002,\n",
        "                             relu_leakiness=0.1)\n",
        "\n",
        "  train(hps , batch_size)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-4ab338ff7fa9>:280: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "WARNING:tensorflow:From <ipython-input-4-4ab338ff7fa9>:107: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step: 0, loss: 2.591, precision: 0.125 \n",
            "step: 100, loss: 1.951, precision: 0.328 \n",
            "step: 200, loss: 1.774, precision: 0.375 \n",
            "step: 300, loss: 1.624, precision: 0.406 \n",
            "step: 400, loss: 1.562, precision: 0.547 \n",
            "step: 500, loss: 1.460, precision: 0.531 \n",
            "step: 600, loss: 1.320, precision: 0.578 \n",
            "step: 700, loss: 1.219, precision: 0.648 \n",
            "step: 800, loss: 1.326, precision: 0.562 \n",
            "step: 900, loss: 1.073, precision: 0.727 \n",
            "step: 1000, loss: 1.336, precision: 0.547 \n",
            "step: 1100, loss: 1.106, precision: 0.664 \n",
            "step: 1200, loss: 1.053, precision: 0.734 \n",
            "step: 1300, loss: 1.031, precision: 0.719 \n",
            "step: 1400, loss: 1.136, precision: 0.648 \n",
            "step: 1500, loss: 1.221, precision: 0.602 \n",
            "step: 1600, loss: 1.047, precision: 0.672 \n",
            "step: 1700, loss: 1.042, precision: 0.711 \n",
            "step: 1800, loss: 1.140, precision: 0.617 \n",
            "step: 1900, loss: 1.138, precision: 0.633 \n",
            "step: 2000, loss: 0.796, precision: 0.805 \n",
            "step: 2100, loss: 0.764, precision: 0.797 \n",
            "step: 2200, loss: 0.920, precision: 0.742 \n",
            "step: 2300, loss: 0.845, precision: 0.773 \n",
            "step: 2400, loss: 0.926, precision: 0.766 \n",
            "step: 2500, loss: 0.893, precision: 0.758 \n",
            "step: 2600, loss: 0.780, precision: 0.758 \n",
            "step: 2700, loss: 0.838, precision: 0.781 \n",
            "step: 2800, loss: 0.800, precision: 0.781 \n",
            "step: 2900, loss: 0.969, precision: 0.750 \n",
            "step: 3000, loss: 0.929, precision: 0.766 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1y3HiS9MtcxW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### You can see *cifar10-train-0~3000* checkpoint files when you run following code, after you train model."
      ]
    },
    {
      "metadata": {
        "id": "fg6E42w4oTB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "a629d7df-7818-4adc-e57d-f49f8d43b5f9"
      },
      "cell_type": "code",
      "source": [
        "!ls train_ckpt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t\tcifar10-train-2300.data-00000-of-00001\n",
            "cifar10-train-1000.data-00000-of-00001\tcifar10-train-2300.index\n",
            "cifar10-train-1000.index\t\tcifar10-train-2300.meta\n",
            "cifar10-train-1000.meta\t\t\tcifar10-train-2400.data-00000-of-00001\n",
            "cifar10-train-100.data-00000-of-00001\tcifar10-train-2400.index\n",
            "cifar10-train-100.index\t\t\tcifar10-train-2400.meta\n",
            "cifar10-train-100.meta\t\t\tcifar10-train-2500.data-00000-of-00001\n",
            "cifar10-train-1100.data-00000-of-00001\tcifar10-train-2500.index\n",
            "cifar10-train-1100.index\t\tcifar10-train-2500.meta\n",
            "cifar10-train-1100.meta\t\t\tcifar10-train-2600.data-00000-of-00001\n",
            "cifar10-train-1200.data-00000-of-00001\tcifar10-train-2600.index\n",
            "cifar10-train-1200.index\t\tcifar10-train-2600.meta\n",
            "cifar10-train-1200.meta\t\t\tcifar10-train-2700.data-00000-of-00001\n",
            "cifar10-train-1300.data-00000-of-00001\tcifar10-train-2700.index\n",
            "cifar10-train-1300.index\t\tcifar10-train-2700.meta\n",
            "cifar10-train-1300.meta\t\t\tcifar10-train-2800.data-00000-of-00001\n",
            "cifar10-train-1400.data-00000-of-00001\tcifar10-train-2800.index\n",
            "cifar10-train-1400.index\t\tcifar10-train-2800.meta\n",
            "cifar10-train-1400.meta\t\t\tcifar10-train-2900.data-00000-of-00001\n",
            "cifar10-train-1500.data-00000-of-00001\tcifar10-train-2900.index\n",
            "cifar10-train-1500.index\t\tcifar10-train-2900.meta\n",
            "cifar10-train-1500.meta\t\t\tcifar10-train-3000.data-00000-of-00001\n",
            "cifar10-train-1600.data-00000-of-00001\tcifar10-train-3000.index\n",
            "cifar10-train-1600.index\t\tcifar10-train-3000.meta\n",
            "cifar10-train-1600.meta\t\t\tcifar10-train-300.data-00000-of-00001\n",
            "cifar10-train-1700.data-00000-of-00001\tcifar10-train-300.index\n",
            "cifar10-train-1700.index\t\tcifar10-train-300.meta\n",
            "cifar10-train-1700.meta\t\t\tcifar10-train-400.data-00000-of-00001\n",
            "cifar10-train-1800.data-00000-of-00001\tcifar10-train-400.index\n",
            "cifar10-train-1800.index\t\tcifar10-train-400.meta\n",
            "cifar10-train-1800.meta\t\t\tcifar10-train-500.data-00000-of-00001\n",
            "cifar10-train-1900.data-00000-of-00001\tcifar10-train-500.index\n",
            "cifar10-train-1900.index\t\tcifar10-train-500.meta\n",
            "cifar10-train-1900.meta\t\t\tcifar10-train-600.data-00000-of-00001\n",
            "cifar10-train-2000.data-00000-of-00001\tcifar10-train-600.index\n",
            "cifar10-train-2000.index\t\tcifar10-train-600.meta\n",
            "cifar10-train-2000.meta\t\t\tcifar10-train-700.data-00000-of-00001\n",
            "cifar10-train-200.data-00000-of-00001\tcifar10-train-700.index\n",
            "cifar10-train-200.index\t\t\tcifar10-train-700.meta\n",
            "cifar10-train-200.meta\t\t\tcifar10-train-800.data-00000-of-00001\n",
            "cifar10-train-2100.data-00000-of-00001\tcifar10-train-800.index\n",
            "cifar10-train-2100.index\t\tcifar10-train-800.meta\n",
            "cifar10-train-2100.meta\t\t\tcifar10-train-900.data-00000-of-00001\n",
            "cifar10-train-2200.data-00000-of-00001\tcifar10-train-900.index\n",
            "cifar10-train-2200.index\t\tcifar10-train-900.meta\n",
            "cifar10-train-2200.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KBDxI6zhoyY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Resnet Eval\n",
        "\n",
        "Before you run this code\n",
        "click Runtime->**restart runtime**\n",
        "\n",
        "(If you want to erase all the local files, then click *RESET ALL RUNTIMES* or **DO NOT CLICK!**)\n",
        "\n",
        "and restart **Define Resnet50 Model**,  **CIFAR10 input**\n",
        "\n",
        "Do not be frightened if you face such an error: \n",
        "`An exception has occurred, use %tb to see the full traceback.  SystemExit`"
      ]
    },
    {
      "metadata": {
        "id": "Wj2qiublo3hU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2228
        },
        "outputId": "45d481a3-1c1d-4cb4-e07a-58a2f309d97f"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf './tensorboard'\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"ResNet Train/Eval module.\n",
        "\"\"\"\n",
        "import time\n",
        "import six\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "eval_data_path = './cifar-10-batches-bin/test_batch.bin'\n",
        "ckpt_dir = './train_ckpt'\n",
        "tensorboard_path = './tensorboard'\n",
        "\n",
        "def evaluate(hps):\n",
        "  \"\"\"Eval loop.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    images, labels = build_input(\n",
        "        'cifar10', './cifar-10-batches-bin/test_batch.bin', hps.batch_size, 'eval')\n",
        "    model = ResNet(hps, images, labels, 'eval')\n",
        "    model.build_graph()\n",
        "\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "\n",
        "    saver = tf.train.Saver() \n",
        "\n",
        "    ######################################\n",
        "    #### FIXME: Make a summary writer ####\n",
        "    #### Use tensorboard_path as path ####\n",
        "    ######################################\n",
        "    summary_writer = tf.summary.FileWriter(tensorboard_path, sess.graph)\n",
        "\n",
        "    try:\n",
        "      ########################################\n",
        "      #### FIXME: Create checkpoint state ####\n",
        "      #### Use ckpt_dir as path           ####\n",
        "      ########################################\n",
        "#       ckpt_state = ???\n",
        "      # Get checkpoint state from checkpoint dir\n",
        "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "      \n",
        "    except tf.errors.OutOfRangeError as e:\n",
        "      tf.logging.error('Cannot restore checkpoint: %s', e)\n",
        "    if not (ckpt_state):\n",
        "      tf.logging.info('No model to eval yet at %s', ckpt_dir)\n",
        "\n",
        "    best_precision = 0.\n",
        "    ############################################################\n",
        "    #### FIXME: Restore checkpoint from first one to latest ####\n",
        "    #### Use ckpt_state                                     ####\n",
        "    ###########################################################\n",
        "    for i in range(len(ckpt_state.all_model_checkpoint_paths)): #range 를 어떻게 해야할지 모르겠다. 아직 Code 분석 없이 돌아가게만 만들었음.10/6 02:13\n",
        "#     for i in range(30):\n",
        "#       tf.logging.info('Loading checkpoint %s', ???)\n",
        "#       saver.restore(sess, ???)\n",
        "      if ckpt_state is not None:\n",
        "        # all_model_checkpoint_paths: the array of checkpoints\n",
        "        tf.logging.info('Loading checkpoint %s', ckpt_state.all_model_checkpoint_paths[i])\n",
        "        # Restore the lastest checkpoint\n",
        "        saver.restore(sess, ckpt_state.all_model_checkpoint_paths[i])\n",
        "\n",
        "      total_prediction, correct_prediction = 0, 0\n",
        "\n",
        "      for _ in six.moves.range(100):\n",
        "        (summaries, loss, predictions, truth, train_step) = sess.run(\n",
        "            [model.summaries, model.cost, model.predictions,\n",
        "             model.labels, model.global_step])\n",
        "\n",
        "        truth = np.argmax(truth, axis=1)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        correct_prediction += np.sum(truth == predictions)\n",
        "        total_prediction += predictions.shape[0]\n",
        "\n",
        "      precision = 1.0 * correct_prediction / total_prediction\n",
        "      best_precision = max(precision, best_precision)\n",
        "\n",
        "\n",
        "      ############################################################\n",
        "      #### FIXME: Add summary of precision and best_precision ####\n",
        "      ############################################################\n",
        "      precisionSum = tf.Summary()\n",
        "      precisionSum.value.add(tag='precisionSum' , simple_value=precision)\n",
        "      summary_writer.add_summary(precisionSum , train_step)\n",
        "      best_precisionSum = tf.Summary()\n",
        "      best_precisionSum.value.add(tag='best_precisionSum' , simple_value=best_precision)\n",
        "      summary_writer.add_summary(best_precisionSum , train_step)\n",
        "\n",
        "      tf.logging.info('loss: %.3f, precision: %.3f, best precision: %.3f' %\n",
        "                      (loss, precision, best_precision))\n",
        "      summary_writer.flush()\n",
        "\n",
        "      tf.logging.info('loss: %.3f, precision: %.3f' %\n",
        "                      (loss, precision))\n",
        "\n",
        "\n",
        "def main(_):\n",
        "\n",
        "  hps = HParams(batch_size=100,\n",
        "                num_classes=10,\n",
        "                min_lrn_rate=0.0001,\n",
        "                lrn_rate=0.1,\n",
        "                num_residual_units=5,\n",
        "                use_bottleneck=False,\n",
        "                weight_decay_rate=0.0002,\n",
        "                relu_leakiness=0.1)\n",
        "\n",
        "  evaluate(hps)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  tf.app.run()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-100\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-100\n",
            "INFO:tensorflow:loss: 1.981, precision: 0.326, best precision: 0.326\n",
            "INFO:tensorflow:loss: 1.981, precision: 0.326\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-200\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-200\n",
            "INFO:tensorflow:loss: 2.080, precision: 0.349, best precision: 0.349\n",
            "INFO:tensorflow:loss: 2.080, precision: 0.349\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-300\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-300\n",
            "INFO:tensorflow:loss: 1.535, precision: 0.456, best precision: 0.456\n",
            "INFO:tensorflow:loss: 1.535, precision: 0.456\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-400\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-400\n",
            "INFO:tensorflow:loss: 1.778, precision: 0.417, best precision: 0.456\n",
            "INFO:tensorflow:loss: 1.778, precision: 0.417\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-500\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-500\n",
            "INFO:tensorflow:loss: 1.628, precision: 0.485, best precision: 0.485\n",
            "INFO:tensorflow:loss: 1.628, precision: 0.485\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-600\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-600\n",
            "INFO:tensorflow:loss: 1.443, precision: 0.543, best precision: 0.543\n",
            "INFO:tensorflow:loss: 1.443, precision: 0.543\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-700\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-700\n",
            "INFO:tensorflow:loss: 1.417, precision: 0.518, best precision: 0.543\n",
            "INFO:tensorflow:loss: 1.417, precision: 0.518\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-800\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-800\n",
            "INFO:tensorflow:loss: 1.346, precision: 0.557, best precision: 0.557\n",
            "INFO:tensorflow:loss: 1.346, precision: 0.557\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-900\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-900\n",
            "INFO:tensorflow:loss: 1.345, precision: 0.581, best precision: 0.581\n",
            "INFO:tensorflow:loss: 1.345, precision: 0.581\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1000\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1000\n",
            "INFO:tensorflow:loss: 1.565, precision: 0.530, best precision: 0.581\n",
            "INFO:tensorflow:loss: 1.565, precision: 0.530\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1100\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1100\n",
            "INFO:tensorflow:loss: 1.468, precision: 0.592, best precision: 0.592\n",
            "INFO:tensorflow:loss: 1.468, precision: 0.592\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1200\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1200\n",
            "INFO:tensorflow:loss: 1.250, precision: 0.629, best precision: 0.629\n",
            "INFO:tensorflow:loss: 1.250, precision: 0.629\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1300\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1300\n",
            "INFO:tensorflow:loss: 1.455, precision: 0.631, best precision: 0.631\n",
            "INFO:tensorflow:loss: 1.455, precision: 0.631\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1400\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1400\n",
            "INFO:tensorflow:loss: 1.523, precision: 0.594, best precision: 0.631\n",
            "INFO:tensorflow:loss: 1.523, precision: 0.594\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1500\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1500\n",
            "INFO:tensorflow:loss: 1.217, precision: 0.643, best precision: 0.643\n",
            "INFO:tensorflow:loss: 1.217, precision: 0.643\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1600\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1600\n",
            "INFO:tensorflow:loss: 1.179, precision: 0.668, best precision: 0.668\n",
            "INFO:tensorflow:loss: 1.179, precision: 0.668\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1700\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1700\n",
            "INFO:tensorflow:loss: 1.427, precision: 0.650, best precision: 0.668\n",
            "INFO:tensorflow:loss: 1.427, precision: 0.650\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1800\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1800\n",
            "INFO:tensorflow:loss: 1.144, precision: 0.668, best precision: 0.668\n",
            "INFO:tensorflow:loss: 1.144, precision: 0.668\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-1900\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-1900\n",
            "INFO:tensorflow:loss: 1.213, precision: 0.672, best precision: 0.672\n",
            "INFO:tensorflow:loss: 1.213, precision: 0.672\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2000\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2000\n",
            "INFO:tensorflow:loss: 1.099, precision: 0.714, best precision: 0.714\n",
            "INFO:tensorflow:loss: 1.099, precision: 0.714\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2100\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2100\n",
            "INFO:tensorflow:loss: 1.080, precision: 0.717, best precision: 0.717\n",
            "INFO:tensorflow:loss: 1.080, precision: 0.717\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2200\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2200\n",
            "INFO:tensorflow:loss: 1.031, precision: 0.684, best precision: 0.717\n",
            "INFO:tensorflow:loss: 1.031, precision: 0.684\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2300\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2300\n",
            "INFO:tensorflow:loss: 1.201, precision: 0.696, best precision: 0.717\n",
            "INFO:tensorflow:loss: 1.201, precision: 0.696\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2400\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2400\n",
            "INFO:tensorflow:loss: 0.987, precision: 0.725, best precision: 0.725\n",
            "INFO:tensorflow:loss: 0.987, precision: 0.725\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2500\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2500\n",
            "INFO:tensorflow:loss: 1.149, precision: 0.691, best precision: 0.725\n",
            "INFO:tensorflow:loss: 1.149, precision: 0.691\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2600\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2600\n",
            "INFO:tensorflow:loss: 0.954, precision: 0.714, best precision: 0.725\n",
            "INFO:tensorflow:loss: 0.954, precision: 0.714\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2700\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2700\n",
            "INFO:tensorflow:loss: 1.333, precision: 0.695, best precision: 0.725\n",
            "INFO:tensorflow:loss: 1.333, precision: 0.695\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2800\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2800\n",
            "INFO:tensorflow:loss: 0.898, precision: 0.728, best precision: 0.728\n",
            "INFO:tensorflow:loss: 0.898, precision: 0.728\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-2900\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-2900\n",
            "INFO:tensorflow:loss: 1.115, precision: 0.752, best precision: 0.752\n",
            "INFO:tensorflow:loss: 1.115, precision: 0.752\n",
            "INFO:tensorflow:Loading checkpoint ./train_ckpt/cifar10-train-3000\n",
            "INFO:tensorflow:Restoring parameters from ./train_ckpt/cifar10-train-3000\n",
            "INFO:tensorflow:loss: 1.036, precision: 0.710, best precision: 0.752\n",
            "INFO:tensorflow:loss: 1.036, precision: 0.710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GQRA4eYdrgdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b2c9ffcd-9f32-45e2-b9d0-e7fc769a228e"
      },
      "cell_type": "code",
      "source": [
        "ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "ckpt_state.all_model_checkpoint_paths"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'./train_ckpt/cifar10-train-100', u'./train_ckpt/cifar10-train-200', u'./train_ckpt/cifar10-train-300', u'./train_ckpt/cifar10-train-400', u'./train_ckpt/cifar10-train-500', u'./train_ckpt/cifar10-train-600', u'./train_ckpt/cifar10-train-700', u'./train_ckpt/cifar10-train-800', u'./train_ckpt/cifar10-train-900', u'./train_ckpt/cifar10-train-1000', u'./train_ckpt/cifar10-train-1100', u'./train_ckpt/cifar10-train-1200', u'./train_ckpt/cifar10-train-1300', u'./train_ckpt/cifar10-train-1400', u'./train_ckpt/cifar10-train-1500', u'./train_ckpt/cifar10-train-1600', u'./train_ckpt/cifar10-train-1700', u'./train_ckpt/cifar10-train-1800', u'./train_ckpt/cifar10-train-1900', u'./train_ckpt/cifar10-train-2000', u'./train_ckpt/cifar10-train-2100', u'./train_ckpt/cifar10-train-2200', u'./train_ckpt/cifar10-train-2300', u'./train_ckpt/cifar10-train-2400', u'./train_ckpt/cifar10-train-2500', u'./train_ckpt/cifar10-train-2600', u'./train_ckpt/cifar10-train-2700', u'./train_ckpt/cifar10-train-2800', u'./train_ckpt/cifar10-train-2900', u'./train_ckpt/cifar10-train-3000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "PlnAphXnUH_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display our graph on tensorboard!"
      ]
    },
    {
      "metadata": {
        "id": "3LodXGDkTuIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "e9139837-5195-415a-cd36-eee746114bf3"
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "#run tensorboard\n",
        "LOG_DIR = './tensorboard'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "#run ngrok\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-11 04:21:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.73.9.93, 52.72.251.164, 54.152.208.69, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.73.9.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  9.42MB/s    in 0.5s    \n",
            "\n",
            "2018-10-11 04:21:18 (9.42 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CGvkGP2AT97B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc85792d-7638-4491-9310-1e1ec4cb62c8"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://1a547024.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}