{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "#unrolled through 28 time steps\n",
    "time_steps=100\n",
    "#hidden LSTM units\n",
    "num_units=128\n",
    "#rows of 28 pixels\n",
    "n_input=12\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "n_classes=6\n",
    "#size of batch\n",
    "batch_size=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data_400.csv\").dropna()\n",
    "test_data = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(test_data[\"type_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[\"type_1\"] = train_data[\"user_id\"].map(str) +\"_\"+ train_data[\"action_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataTemp : CSV input/out 섞여있는 데이터 \n",
    "# period : 자를 timeSteps\n",
    "# onehot : default True, 원핫으로 return 할지 말지 결정\n",
    "# return : 다 만들어진 X , Y\n",
    "def dataPreprocessing(dataTemp , period , onehot = True , dropCol = True ):\n",
    "    encoding = {'jog': 0, 'dws': 1, 'ups': 2, 'wlk': 3, 'sit': 4, 'std': 5}\n",
    "    retY = pd.Series([])\n",
    "#     retY = np.array([])\n",
    "    retX = dataTemp\n",
    "    temptype = -1\n",
    "    cnt = 0\n",
    "    dataTemp[\"type_1\"] = dataTemp[\"user_id\"].map(str) +\"_\"+ dataTemp[\"action_id\"]\n",
    "    for dataIter in range(dataTemp[\"action_id\"].size//period):\n",
    "        retY[cnt] = encoding[dataTemp[\"action_id\"].iloc[dataIter*period][:3]]\n",
    "        comp = dataTemp[\"type_1\"].iloc[dataIter*period]\n",
    "        if comp == temptype:\n",
    "            cnt += 1\n",
    "        else:\n",
    "#             print(cnt)\n",
    "            temptype = comp\n",
    "            retX = retX.drop(retX.index[dataIter:dataIter+period])\n",
    "    if onehot:\n",
    "        retY = pd.get_dummies(retY)\n",
    "    if dropCol:\n",
    "        retX = retX.drop([\"action_id\"] , axis = 1)\n",
    "        retX = retX.drop([\"user_id\"] , axis = 1)\n",
    "        retX = retX.drop([\"time_50hz\"] , axis = 1)\n",
    "        retX = retX.drop([\"type_1\"] , axis = 1)\n",
    "    return retX , retY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X , train_Y = dataPreprocessing(train_data , time_steps , True)\n",
    "test_X , test_Y = dataPreprocessing(test_data , time_steps , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1209003, 12), (12090, 6), (167862, 12), (1678, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape , train_Y.shape , test_X.shape , test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = {'jog': 0, 'dws': 1, 'ups': 2, 'wlk': 3, 'sit': 4, 'std': 5}\n",
    "# y_train = train_data[\"action_id\"].apply(lambda x:encoding[x[:3]])\n",
    "# y_test= test_data[\"action_id\"].apply(lambda x:encoding[x[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = pd.get_dummies(y_train)\n",
    "# y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_input = train_data.drop([\"action_id\"] , axis = 1)\n",
    "# temp_input = temp_input.drop([\"user_id\"] , axis = 1)\n",
    "# temp_input = temp_input.drop([\"time_50hz\"] , axis = 1)\n",
    "# test_input = test_data.drop([\"action_id\"] , axis = 1)\n",
    "# test_input = test_input.drop([\"user_id\"] , axis = 1)\n",
    "# test_input = test_input.drop([\"time_50hz\"] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_X = temp_input[:1240503]\n",
    "# temp_Y = y_train[:1240503]\n",
    "# test_X = test_input[:170000]\n",
    "# test_Y = y_test[:170000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "x=tf.placeholder(tf.float32 ,[None,time_steps , n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(tf.int32 ,[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tf.unstack(x ,time_steps,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-c3e3d217c690>:2: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = tf.argmax(prediction,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ,  Test Acc 0.27937498688697815 , los 6.747722625732422 \n",
      "Epoch 2 ,  Test Acc 0.27687498927116394 , los 4.385247230529785 \n",
      "Epoch 3 ,  Test Acc 0.2787500023841858 , los 4.353761196136475 \n",
      "Epoch 4 ,  Test Acc 0.33812499046325684 , los 2.7773358821868896 \n",
      "Epoch 5 ,  Test Acc 0.35062500834465027 , los 2.194559097290039 \n",
      "Epoch 6 ,  Test Acc 0.41999998688697815 , los 2.0462234020233154 \n",
      "Epoch 7 ,  Test Acc 0.47062501311302185 , los 1.6268396377563477 \n",
      "Epoch 8 ,  Test Acc 0.5099999904632568 , los 1.4484905004501343 \n",
      "Epoch 9 ,  Test Acc 0.6162499785423279 , los 1.3106616735458374 \n",
      "Epoch 10 ,  Test Acc 0.6462500095367432 , los 1.2842544317245483 \n",
      "Epoch 11 ,  Test Acc 0.5181249976158142 , los 1.483980655670166 \n",
      "Epoch 12 ,  Test Acc 0.6612499952316284 , los 1.2103365659713745 \n",
      "Epoch 13 ,  Test Acc 0.659375011920929 , los 1.2302844524383545 \n",
      "Epoch 14 ,  Test Acc 0.6631249785423279 , los 1.1532647609710693 \n",
      "Epoch 15 ,  Test Acc 0.6812499761581421 , los 1.1384409666061401 \n",
      "Epoch 16 ,  Test Acc 0.6768749952316284 , los 1.0716631412506104 \n",
      "Epoch 17 ,  Test Acc 0.6737499833106995 , los 1.1314865350723267 \n",
      "Epoch 18 ,  Test Acc 0.6875 , los 1.1201592683792114 \n",
      "Epoch 19 ,  Test Acc 0.7049999833106995 , los 1.0858765840530396 \n",
      "Epoch 20 ,  Test Acc 0.7093750238418579 , los 1.1008355617523193 \n"
     ]
    }
   ],
   "source": [
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "test_X_temp = np.array(test_X[:160000]).reshape(-1,time_steps , n_input)\n",
    "epoch = 20\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "for ep  in range(1,epoch+1):    \n",
    "    iter=0\n",
    "    while iter<99:\n",
    "        batch_x = train_X[iter*batch_size*time_steps:(iter+1)*batch_size*time_steps]\n",
    "        batch_y = train_Y[iter*batch_size:(iter+1)*batch_size]\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})  \n",
    "#             if iter % 10 == 0:\n",
    "#                 acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "#                 los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "        iter=iter+1\n",
    "    if ep % 1 == 0:            \n",
    "        test_acc = sess.run(accuracy, feed_dict={x: test_X_temp, y: test_Y[:1600]})\n",
    "        test_los = sess.run(loss, feed_dict={x: test_X_temp, y: test_Y[:1600]})\n",
    "        print(\"Epoch {} ,  Test Acc {} , los {} \".format(ep,test_acc , test_los))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_eval 에 분류를 위한 type_1 을 넣는데.\n",
    "test_data_eval = test_data\n",
    "test_data_eval[\"type_1\"] = test_data[\"user_id\"].map(str) +\"_\"+ test_data[\"action_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 의 종류를 가지고 와서 나중에 loop 를 돌면서 accuracy 를 만들것임\n",
    "dic_testType = list(set(test_data_eval[\"type_1\"]))\n",
    "# dic_testType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict :  [3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 3]  Real Y :  Int64Index([3], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# full loop 만들어보기\n",
    "\n",
    "\n",
    "# 한 타입만 일단 가져와보고\n",
    "tempX = test_data_eval[(test_data_eval[\"type_1\"] == '2_wlk_15')]\n",
    "# matrix 에 맞게 뒤에 잘라내줌.\n",
    "tempX = tempX[:(tempX.shape[0] // time_steps)*time_steps]\n",
    "# 여기에서 데이터를 X,Y 로 먼저 나눠준다. onehot 해뒀고 필요없는 column 까지 다 Drop\n",
    "evalTestX , evalTestY = dataPreprocessing(tempX , time_steps , True , True)\n",
    "evalTestX = np.array(evalTestX).reshape(-1,time_steps , n_input)\n",
    "# one hot 에 하나만 있기때문에 나머지 다 넣어주고 순서 원래대로 변경\n",
    "#         for i in range(6):\n",
    "#             if i not in set(evalTestY):\n",
    "#                 evalTestY[i] = 0\n",
    "#         evalTestY=evalTestY[[0,1,2,3,4,5]]\n",
    "print('predict : ' , sess.run(Yhat , feed_dict={x:evalTestX}) , \" Real Y : \" , evalTestY.columns)\n",
    "#     print(evalTestX.shape , evalTestY.shape , ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ite :  1_wlk_8  predict :  [3 3 0 3 2 3 3 3 3 3 3 2 3 2 3 1 2 3 2 3 3 3 3 3 2 2 3 3 3 3 3 3 2 3 3 3 3]  Real Y :  3\n",
      "ite :  1_sit_5  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]  Real Y :  4\n",
      "ite :  2_jog_9  predict :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]  Real Y :  0\n",
      "ite :  3_wlk_15  predict :  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]  Real Y :  3\n",
      "ite :  3_wlk_8  predict :  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]  Real Y :  3\n",
      "ite :  3_ups_4  predict :  [3 2 2 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 2 2 3 3 3 3 3 3 2 3 2 3 3 3 3 3 3 3]  Real Y :  2\n",
      "ite :  1_dws_11  predict :  [5 3 3 2 1]  Real Y :  1\n",
      "ite :  2_jog_16  predict :  [0 0 0 0 0 0 0 0 0]  Real Y :  0\n",
      "ite :  2_ups_12  predict :  [0 3]  Real Y :  2\n",
      "ite :  1_dws_2  predict :  [1 3 1 4 3 1 5 3 3 4 3 1 3 0 3 3 1 5 5 3]  Real Y :  1\n",
      "ite :  3_jog_9  predict :  [0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 3 0 0 3 0 0 0 0 3 3 3 0 0 2 0 0 0 0 2 0 0 0\n",
      " 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0]  Real Y :  0\n",
      "ite :  2_dws_2  predict :  [3 3 3 1 3 3 3 3 3 3 3 3 3 3 1 3 3]  Real Y :  1\n",
      "ite :  2_ups_3  predict :  [0 2 3 3 0 3 0 2]  Real Y :  2\n",
      "ite :  3_dws_1  predict :  [1 1 3 2 1 1 3 3 1 3 3 3 3 2 3 1 3 1 3 2 5 3 2 1 3 3 1 1]  Real Y :  1\n",
      "ite :  3_dws_2  predict :  [3 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 1 3 3 3 1 2 3 3 1 3 3]  Real Y :  1\n",
      "ite :  1_ups_12  predict :  [3 2 3 5 3 2 3 5]  Real Y :  2\n",
      "ite :  2_dws_11  predict :  [1 3 3 2]  Real Y :  1\n",
      "ite :  1_std_6  predict :  [3 1 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]  Real Y :  5\n",
      "ite :  3_dws_11  predict :  [2 1 2 3 2 1 3 3 2 2]  Real Y :  1\n",
      "ite :  2_sit_13  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]  Real Y :  4\n",
      "ite :  3_std_6  predict :  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 3]  Real Y :  5\n",
      "ite :  2_wlk_8  predict :  [3 3 2 2 2 1 3 3 3 1 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 2 3 3 2 2 3 2 2 2 3 3]  Real Y :  3\n",
      "ite :  1_jog_9  predict :  [0 0 0 2 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 3 3 2 2 0 0 0 2 0 0 0 0 0 0]  Real Y :  0\n",
      "ite :  2_wlk_15  predict :  [3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 3]  Real Y :  3\n",
      "ite :  1_dws_1  predict :  [5 5 1 0 2 5 3 5 3 1 5 5 3 3 4 3 5 3 2 1]  Real Y :  1\n",
      "ite :  2_std_6  predict :  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5]  Real Y :  5\n",
      "ite :  3_ups_3  predict :  [3 3 3 3 3 2 3 2 2 3 2 3 3 3 3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 3 3 3]  Real Y :  2\n",
      "ite :  2_ups_4  predict :  [0 3 2 3 0 3 2 2 3 2 2 3 3 0]  Real Y :  2\n",
      "ite :  1_ups_3  predict :  [3 2 2 3 2 3 3 3 3 3 5 5 2 2 3 3 2 5]  Real Y :  2\n",
      "ite :  3_sit_13  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4]  Real Y :  4\n",
      "ite :  3_jog_16  predict :  [0 0 0 0 0 3 3 0 0 0 0 0 0 0 3]  Real Y :  0\n",
      "ite :  3_std_14  predict :  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]  Real Y :  5\n",
      "ite :  3_ups_12  predict :  [2 3 3 3 3 3 3 3 3 3 2 3 3 3 3]  Real Y :  2\n",
      "ite :  1_sit_13  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4]  Real Y :  4\n",
      "ite :  2_sit_5  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]  Real Y :  4\n",
      "ite :  1_wlk_15  predict :  [3 2 3 2 3 2 2 3 3 3 2 2 3 3 3 3 2 3 3 3 3 3 3]  Real Y :  3\n",
      "ite :  1_wlk_7  predict :  [5 5 2 2 3 0 1 2 2 3 3 3 1 2 2 2 3 3 1 3 3 2 3 3 3 3 3 2 2 3 3 3 2 3 2 2 3\n",
      " 3 3 3 3 3 3 2 3 3]  Real Y :  3\n",
      "ite :  2_wlk_7  predict :  [3 3 3 3 2 3 3 3 2 2 3 2 2 2 3 3 2 3 3 3 2 3 3 3 2 2 3 2 2 2 3 2 3 3 3 3 3\n",
      " 2 3 3 3 2 3 3 2 2 3 3 3 3 2 3 3]  Real Y :  3\n",
      "ite :  3_sit_5  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]  Real Y :  4\n",
      "ite :  1_std_14  predict :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]  Real Y :  5\n",
      "ite :  3_wlk_7  predict :  [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]  Real Y :  3\n",
      "ite :  1_ups_4  predict :  [3 2 3 3 3 3 5 3 2 3 4 3 2 5 3 3 3 3 3 3 2 3 3 2 4]  Real Y :  2\n",
      "ite :  1_jog_16  predict :  [0 0 1 0 0 0]  Real Y :  0\n",
      "ite :  2_std_14  predict :  [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5]  Real Y :  5\n",
      "ite :  2_dws_1  predict :  [1 1 3 1 3 1 1 3 1 1 3 2 3 3]  Real Y :  1\n"
     ]
    }
   ],
   "source": [
    "# full loop 만들어보기\n",
    "y_pred = []\n",
    "y_true = []\n",
    "# >>> accuracy_score(y_true, y_pred)\n",
    "for ite in dic_testType:\n",
    "    # 한 타입만 일단 가져와보고\n",
    "    tempX = test_data_eval[(test_data_eval[\"type_1\"] == ite)]\n",
    "    # matrix 에 맞게 뒤에 잘라내줌.\n",
    "    tempX = tempX[:(tempX.shape[0] // time_steps)*time_steps]\n",
    "    # 여기에서 데이터를 X,Y 로 먼저 나눠준다. onehot 해뒀고 필요없는 column 까지 다 Drop\n",
    "    evalTestX , evalTestY = dataPreprocessing(tempX , time_steps , True , True)\n",
    "    evalTestX = np.array(evalTestX).reshape(-1,time_steps , n_input)\n",
    "    # one hot 에 하나만 있기때문에 나머지 다 넣어주고 순서 원래대로 변경\n",
    "#         for i in range(6):\n",
    "#             if i not in set(evalTestY):\n",
    "#                 evalTestY[i] = 0\n",
    "#         evalTestY=evalTestY[[0,1,2,3,4,5]]\n",
    "    yh = sess.run(Yhat , feed_dict={x:evalTestX})\n",
    "    pre = Counter(yh).most_common(1)[0][0]\n",
    "    tru = evalTestY.columns[0]\n",
    "    y_pred.append(pre)\n",
    "    y_true.append(tru)\n",
    "    print(\"ite : \" , ite , ' predict : ' , yh , \" Real Y : \" , tru)\n",
    "#     print('predict : ' , pre , \" Real Y : \" , tru)\n",
    "#     print(evalTestX.shape , evalTestY.shape , ite)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = test_data_eval[(test_data_eval[\"type_1\"] == '2_dws_2')]\n",
    "# reshape 이 자동으로 되게 time_steps 로 잘라줌\n",
    "temp = temp[:(temp.shape[0] // time_steps)*time_steps]\n",
    "xx , yy = dataPreprocessing(temp , time_steps , True , True)\n",
    "xx = np.array(xx).reshape(-1,time_steps , n_input)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: xx, y: yy})\n",
    "    test_los = sess.run(loss, feed_dict={x: xx, y: yy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputDic:\n",
    "    temp = test_data_eval[(test_data_eval[\"type_1\"] == i)]\n",
    "    # reshape 이 자동으로 되게 time_steps 로 잘라줌\n",
    "    temp = temp[:(temp.shape[0] // time_steps)*time_steps]\n",
    "    xx , yy = dataPreprocessing(temp , time_steps , True , True)\n",
    "    xx = np.array(xx).reshape(-1,time_steps , n_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_temp = np.array(test_X[:160000]).reshape(-1,time_steps , n_input)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: test_X_temp, y: test_Y[:1600]})\n",
    "    test_los = sess.run(loss, feed_dict={x: test_X_temp, y: test_Y[:1600]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
